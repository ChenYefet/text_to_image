# ──────────────────────────────────────────────────────────────────────────────
# Text-to-Image with Prompt Assist — Environment Configuration
# ──────────────────────────────────────────────────────────────────────────────
# Copy this file to .env and adjust the values to match your local environment.
# All variables are prefixed with TEXT_TO_IMAGE_ to avoid collisions.
#
# Every variable below maps to a field in configuration.ApplicationConfiguration.
# See Section 17 (Configuration Requirements) of the v5.0.0 specification for
# the canonical reference.
# ──────────────────────────────────────────────────────────────────────────────

# ── Application settings ─────────────────────────────────────────────────────

# Host address on which this API service will listen.
# Use 0.0.0.0 to expose the service on all network interfaces.
TEXT_TO_IMAGE_APPLICATION_HOST=127.0.0.1

# Port on which this API service will listen.
TEXT_TO_IMAGE_APPLICATION_PORT=8000

# Comma-separated list of allowed CORS origins.
# Leave empty to disable CORS (no Access-Control-Allow-Origin header).
# Example: ["http://localhost:3000","https://myapp.example.com"]
TEXT_TO_IMAGE_CORS_ALLOWED_ORIGINS=[]

# Log level for structured JSON logging: DEBUG, INFO, WARNING, ERROR, CRITICAL.
TEXT_TO_IMAGE_LOG_LEVEL=INFO

# Rate limit for inference endpoints (prompt enhancement and image generation).
# Uses the format 'count/period' where period is: second, minute, hour, day.
# Set to '0/second' to disable rate limiting.
TEXT_TO_IMAGE_RATE_LIMIT=10/minute

# ── Language model (llama.cpp) settings ──────────────────────────────────────

# File path of the GGUF language model used by the llama.cpp server.
# This variable is reference only — it is not consumed by the API service at
# runtime (the service communicates with llama.cpp via HTTP), but is declared
# for tooling visibility and deployment automation (§17 of the specification).
# The --model argument for llama.cpp should point to this file path.
#   Windows example:    C:\Models\Meta-Llama-3-8B-Instruct.Q4_K_M.gguf
#   Linux/macOS example: ~/Models/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf
# TEXT_TO_IMAGE_LANGUAGE_MODEL_PATH=

# Base URL of the llama.cpp server running in OpenAI-compatible mode.
# The service appends /v1/chat/completions to this URL.
TEXT_TO_IMAGE_LANGUAGE_MODEL_SERVER_BASE_URL=http://localhost:8080

# Maximum number of seconds to wait for a response from the language model server.
TEXT_TO_IMAGE_TIMEOUT_FOR_LANGUAGE_MODEL_REQUESTS_IN_SECONDS=120.0

# Sampling temperature for prompt enhancement (0.0 = deterministic, higher = more creative).
TEXT_TO_IMAGE_LANGUAGE_MODEL_TEMPERATURE=0.7

# Maximum number of tokens the language model may generate for an enhanced prompt.
TEXT_TO_IMAGE_LANGUAGE_MODEL_MAXIMUM_TOKENS=512

# System prompt sent to the llama.cpp server on every prompt enhancement request.
# Controls the enhancement style and output format. Must be a non-empty string.
# Uncomment the line below to override the built-in default.
# TEXT_TO_IMAGE_LANGUAGE_MODEL_SYSTEM_PROMPT=You are an expert at enhancing text-to-image prompts. Transform the user's simple prompt into a detailed, visually descriptive prompt. Add artistic style, lighting, composition, and quality modifiers. Return only the enhanced prompt, nothing else.

# Maximum number of connections maintained in the httpx connection pool for the
# llama.cpp HTTP client. Increase if deploying multiple service instances against
# a single llama.cpp server or if concurrency is increased.
TEXT_TO_IMAGE_LANGUAGE_MODEL_CONNECTION_POOL_SIZE=10

# Maximum response body size in bytes the service will read from the llama.cpp
# server. Responses exceeding this limit are treated as upstream failures (HTTP 502).
# Default is 1 MB (1,048,576 bytes).
TEXT_TO_IMAGE_LANGUAGE_MODEL_MAXIMUM_RESPONSE_BYTES=1048576

# ── Stable Diffusion settings ────────────────────────────────────────────────

# HuggingFace model ID or local path for the Stable Diffusion model.
# The model is downloaded automatically from HuggingFace Hub on first run.
TEXT_TO_IMAGE_STABLE_DIFFUSION_MODEL_ID=stable-diffusion-v1-5/stable-diffusion-v1-5

# Hugging Face model revision identifier (a specific commit hash or branch name).
# Pinning to a specific commit hash ensures identical model weights across all
# deployments. Use "main" to track the latest revision (not recommended for
# production). Recommended pinned revision for evaluation environments:
# 39593d5650112b4cc580433f6b0435385882d819
TEXT_TO_IMAGE_STABLE_DIFFUSION_MODEL_REVISION=39593d5650112b4cc580433f6b0435385882d819

# Device for Stable Diffusion inference: "auto", "cpu", or "cuda".
# "auto" selects CUDA when available, otherwise falls back to CPU.
TEXT_TO_IMAGE_STABLE_DIFFUSION_DEVICE=auto

# Number of denoising steps for Stable Diffusion inference.
# Higher values produce better quality but take longer.
TEXT_TO_IMAGE_STABLE_DIFFUSION_INFERENCE_STEPS=20

# Classifier-free guidance scale for Stable Diffusion.
# Higher values follow the prompt more closely; lower values are more creative.
TEXT_TO_IMAGE_STABLE_DIFFUSION_GUIDANCE_SCALE=7.0

# Enable the Stable Diffusion content safety checker (true/false).
# Disabling it removes content filtering from generated images.
TEXT_TO_IMAGE_STABLE_DIFFUSION_SAFETY_CHECKER=true

# Base timeout (seconds) for generating one 512x512 image.
# The service auto-scales: base × n_images × (w × h) / (512 × 512), with a 30x
# multiplier on CPU.  The default (60s) works on GPU; CPU operators on slow
# hardware should increase this to match their observed single-image time.
TEXT_TO_IMAGE_STABLE_DIFFUSION_INFERENCE_TIMEOUT_PER_UNIT_SECONDS=60.0

# ── Admission control and resilience settings ────────────────────────────────

# Maximum number of image generation inference operations permitted to execute
# concurrently within a single service instance.  When this limit is reached,
# additional requests are rejected immediately with HTTP 429 (service_busy).
# A value of 1 is strongly recommended for CPU-only deployments.
TEXT_TO_IMAGE_IMAGE_GENERATION_MAXIMUM_CONCURRENCY=1

# Value (in seconds) of the Retry-After response header on HTTP 429 responses
# caused by admission control (error code: service_busy).  This indicates global
# GPU/CPU capacity saturation.  A shorter backoff (default 30 s) is appropriate
# because the condition is transient — it resolves as soon as the current
# inference operation completes.
TEXT_TO_IMAGE_RETRY_AFTER_BUSY_SECONDS=30

# Value (in seconds) of the Retry-After response header on HTTP 429 responses
# caused by per-IP rate limiting (error code: rate_limit_exceeded).  This indicates
# that a single client is sending requests too frequently.  A longer backoff
# (default 60 s) is appropriate because the rate-limit window must expire before
# the client can send additional requests.
TEXT_TO_IMAGE_RETRY_AFTER_RATE_LIMIT_SECONDS=60

# Value (in seconds) of the Retry-After response header on HTTP 503 responses.
# Operators should tune this to reflect the expected service initialisation duration.
TEXT_TO_IMAGE_RETRY_AFTER_NOT_READY_SECONDS=10

# Maximum request payload size in bytes.  Requests exceeding this limit are
# rejected with HTTP 413 before the body is fully read.  Default is 1 MB.
TEXT_TO_IMAGE_MAXIMUM_REQUEST_PAYLOAD_BYTES=1048576

# Maximum end-to-end duration in seconds for any single HTTP request.
# Requests exceeding this ceiling are aborted with HTTP 504 (request_timeout).
# This value should be less than or equal to the reverse proxy's read timeout.
TEXT_TO_IMAGE_TIMEOUT_FOR_REQUESTS_IN_SECONDS=300
